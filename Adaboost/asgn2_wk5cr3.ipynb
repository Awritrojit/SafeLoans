{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = turicreate.SFrame('lending-club-data.sframe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans.remove_column('bad_loans')\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of safe loans                 : 0.5022361744216048\n",
      "Percentage of risky loans                : 0.4977638255783951\n",
      "Total number of loans in our new dataset : 46508\n"
     ]
    }
   ],
   "source": [
    "safe_loans_raw = loans[loans[target] == 1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "risky_loans = risky_loans_raw\n",
    "safe_loans = safe_loans_raw.sample(percentage, seed=1)\n",
    "loans_data = risky_loans_raw.append(safe_loans)\n",
    "\n",
    "print(\"Percentage of safe loans                 :\", len(safe_loans) / float(len(loans_data)))\n",
    "print(\"Percentage of risky loans                :\", len(risky_loans) / float(len(loans_data)))\n",
    "print(\"Total number of loans in our new dataset :\", len(loans_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_data = risky_loans.append(safe_loans)\n",
    "for feature in features:\n",
    "    loans_data_one_hot_encoded = loans_data[feature].apply(lambda x: {x: 1})    \n",
    "    loans_data_unpacked = loans_data_one_hot_encoded.unpack(column_name_prefix=feature)\n",
    "    \n",
    "    # Change None's to 0's\n",
    "    for column in loans_data_unpacked.column_names():\n",
    "        loans_data_unpacked[column] = loans_data_unpacked[column].fillna(0)\n",
    "\n",
    "    loans_data = loans_data.remove_column(feature)\n",
    "    loans_data = loans_data.add_columns(loans_data_unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade.A',\n",
       " 'grade.B',\n",
       " 'grade.C',\n",
       " 'grade.D',\n",
       " 'grade.E',\n",
       " 'grade.F',\n",
       " 'grade.G',\n",
       " 'term. 36 months',\n",
       " 'term. 60 months',\n",
       " 'home_ownership.MORTGAGE',\n",
       " 'home_ownership.OTHER',\n",
       " 'home_ownership.OWN',\n",
       " 'home_ownership.RENT',\n",
       " 'emp_length.1 year',\n",
       " 'emp_length.10+ years',\n",
       " 'emp_length.2 years',\n",
       " 'emp_length.3 years',\n",
       " 'emp_length.4 years',\n",
       " 'emp_length.5 years',\n",
       " 'emp_length.6 years',\n",
       " 'emp_length.7 years',\n",
       " 'emp_length.8 years',\n",
       " 'emp_length.9 years',\n",
       " 'emp_length.< 1 year',\n",
       " 'emp_length.n/a']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = loans_data.column_names()\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = loans_data.random_split(0.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    \n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    weighted_mistakes_all_negative = total_weight_positive\n",
    "    \n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    weighted_mistakes_all_positive = total_weight_negative\n",
    "    \n",
    "    if weighted_mistakes_all_positive < weighted_mistakes_all_negative:\n",
    "        return (weighted_mistakes_all_positive, +1)\n",
    "    elif weighted_mistakes_all_positive > weighted_mistakes_all_negative:\n",
    "        return (weighted_mistakes_all_negative, -1)\n",
    "    else:\n",
    "        return (weighted_mistakes_all_positive, +1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "       \n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] == 1]\n",
    "                    \n",
    "        # DIFFERENT HERE\n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        \n",
    "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(left_split[target], \n",
    "                                                                                 left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], \n",
    "                                                                                   right_data_weights)\n",
    "        \n",
    "        # DIFFERENT HERE\n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        \n",
    "        error = (left_weighted_mistakes + right_weighted_mistakes) / sum(data_weights)\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    leaf['prediction'] = best_class\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "\n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print(\"Stopping condition 1 reached.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print(\"Stopping condition 2 reached.\")\n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print(\"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split)))\n",
    "\n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(\n",
    "        left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(\n",
    "        right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Split on feature grade.A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9122 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (101 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Split on feature grade.D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (23300 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4701 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = turicreate.SArray([1.0 for i in range(len(train_data))])\n",
    "\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test failed... try again!')\n",
    "    print('Number of nodes found:', count_nodes(small_data_decision_tree))\n",
    "    print('Number of nodes that should be there: 7' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print(\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x))\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3981042654028436"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.RENT. (20514, 16710)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (20514 data points).\n",
      "Split on feature grade.F. (19613, 901)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19613 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (901 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16710 data points).\n",
      "Split on feature grade.D. (13315, 3395)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13315 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3395 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = turicreate.SArray([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48124865678057166"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data\n",
    "    alpha = turicreate.SArray([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in range(num_tree_stumps):\n",
    "        print('=====================================================')\n",
    "        print('Adaboost Iteration %d' % t)\n",
    "        print('=====================================================')\n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "       \n",
    "        weighted_error = sum(alpha*is_wrong)/sum(alpha)\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        \n",
    "        weight = 0.5*log((1 - weighted_error) / weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        \n",
    "        alpha = alpha*adjustment\n",
    "        alpha = alpha/sum(alpha)\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print(\"(leaf, label: %s)\" % tree['prediction'])\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print('                       root')\n",
    "    print('         |---------------|----------------|')\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name))))\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('    (%s)                 (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term. 36 months == 0]            [term. 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]                    [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15802933659263743, 0.1768236329364191]\n"
     ]
    }
   ],
   "source": [
    "print(stump_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = turicreate.SArray([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Accumulate predictions on scores array\n",
    "        scores += stump_weights[i]*predictions\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10-component ensemble = 0.6203145196036192\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "accuracy = turicreate.evaluation.accuracy(test_data[target], predictions)\n",
    "print('Accuracy of 10-component ensemble = %s' % accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15802933659263743,\n",
       " 0.1768236329364191,\n",
       " 0.09311888971129693,\n",
       " 0.07288885525840554,\n",
       " 0.06706306914118143,\n",
       " 0.06456916961644447,\n",
       " 0.05456055779178564,\n",
       " 0.04351093673362621,\n",
       " 0.02898871150041245,\n",
       " 0.02596250969152032]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.4 years. (34593, 2631)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34593 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2631 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.2 years. (33652, 3572)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33652 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3572 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.OWN. (34149, 3075)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34149 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3075 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.4216365785514722\n",
      "Iteration 2, training error = 0.43343004513217276\n",
      "Iteration 3, training error = 0.4000376101439931\n",
      "Iteration 4, training error = 0.4000376101439931\n",
      "Iteration 5, training error = 0.3847249086610789\n",
      "Iteration 6, training error = 0.3846174511068128\n",
      "Iteration 7, training error = 0.3827638082957232\n",
      "Iteration 8, training error = 0.3846174511068128\n",
      "Iteration 9, training error = 0.3827638082957232\n",
      "Iteration 10, training error = 0.3844831291639802\n",
      "Iteration 11, training error = 0.3827369439071566\n",
      "Iteration 12, training error = 0.38144745325596385\n",
      "Iteration 13, training error = 0.3815280464216635\n",
      "Iteration 14, training error = 0.38056092843326883\n",
      "Iteration 15, training error = 0.3805071996561358\n",
      "Iteration 16, training error = 0.378223726627982\n",
      "Iteration 17, training error = 0.3782774554051149\n",
      "Iteration 18, training error = 0.3784117773479475\n",
      "Iteration 19, training error = 0.3780625402965828\n",
      "Iteration 20, training error = 0.3787610143993123\n",
      "Iteration 21, training error = 0.3795669460563078\n",
      "Iteration 22, training error = 0.3788953363421449\n",
      "Iteration 23, training error = 0.3788953363421449\n",
      "Iteration 24, training error = 0.3787610143993123\n",
      "Iteration 25, training error = 0.3788953363421449\n",
      "Iteration 26, training error = 0.3789759295078444\n",
      "Iteration 27, training error = 0.379110251450677\n",
      "Iteration 28, training error = 0.37892220073071137\n",
      "Iteration 29, training error = 0.3790296582849775\n",
      "Iteration 30, training error = 0.37873415001074573\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in range(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - turicreate.evaluation.accuracy(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print(\"Iteration %s, training error = %s\" % (n, error_all[n-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
